{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UsG-Hz2HCcO6",
    "outputId": "08decb86-9a0b-4077-d514-bccb022ed014"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'QG_demo'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/DIKLA2322/QG_demo.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZioXMvwhZhEw"
   },
   "source": [
    "## Init_QG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aweZgxXBDsOQ",
    "outputId": "54c1a889-4bb7-4edb-83c5-802c6842a3aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FARi6xuQ4IZ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EklUTzhezVqO",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!apt install htop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bUO9ffkCUD6"
   },
   "source": [
    "## Init_TC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcJR_k6-CZHC",
    "outputId": "328d5791-6113-48ca-e900-6af79219f33f",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U torch==1.8.0 torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlD3VPFWIptu",
    "outputId": "f03aef5b-edd6-4126-d55e-cde5a7b2292e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!python -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "je5ed4ZtCacA",
    "outputId": "2274ffa9-1d6f-4d06-c84c-0dc0fe24d3d1",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!apt install htop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDLcAEMOYa5S"
   },
   "source": [
    "## Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfNX0t_AVGQB",
    "outputId": "ea75c89b-ff9f-466d-8170-0db47332fe4d",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cd QG_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oNZEehJYhBs",
    "outputId": "9fbd5266-8ec4-4711-d78c-b88d45093382",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!git fetch --all\n",
    "!git reset --hard origin/master\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIjYRmvI7OrE"
   },
   "source": [
    "## Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87qYoujeYgtN",
    "outputId": "c49565ce-d273-4722-f612-11bee303ead4",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cd text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3pDcCxbTEhz"
   },
   "source": [
    "### Google Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxIMMySlcxRi",
    "outputId": "8b9a987f-2ac5-432c-af6e-66477d139aac",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q29f4lJjTM4z",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBSEaRpsZW63",
    "outputId": "6539f2b6-98c2-44d9-ca07-95630cd4a08d",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 去停用词\n",
    "def get_stop_words():\n",
    "  from nltk.corpus import stopwords\n",
    "  stop_words = stopwords.words('english')\n",
    "  for w in ['!',',','.','?','-s','-ly','','s']:\n",
    "    stop_words.append(w)\n",
    "  return stop_words\n",
    "\n",
    "stop_words = get_stop_words()  # 加载停用词表\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_ThFqu-UeoP",
    "outputId": "c411bff5-c34d-4f03-9245-630bfc55f4e7",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "# Create a set of frequent words\n",
    "stoplist = stop_words\n",
    "\n",
    "text_corpus = open('./data/text.tsv', 'r', encoding=\"ISO-8859-1\", errors='ignore').read()\n",
    "# print(text_corpus)\n",
    "tokens_list = []\n",
    "processed_corpus = []\n",
    "text_corpus = text_corpus.split(\"\\n\")\n",
    "# print(text_corpus)\n",
    "for line in text_corpus:\n",
    "  # print(text_corpus)\n",
    "  tokens = word_tokenize(line)   #分词\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "  # print(tokens)\n",
    "  tokens_list.append(tokens)\n",
    "# print('【NLTK分词结果：】')\n",
    "print(tokens_list)\n",
    "\n",
    "interpunctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']   #定义标点符号列表\n",
    "for tokens in tokens_list:\n",
    "  processed_tokens = [word for word in tokens if word not in interpunctuations]   #去除标点符号\n",
    "  filtered_words = [word for word in processed_tokens if word not in stoplist]\n",
    "# print('\\n【NLTK分词后去除符号结果：】')\n",
    "  processed_corpus.append(filtered_words)\n",
    "print(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvoCD65cUgh2",
    "outputId": "7877f7dd-40bf-48fa-ba2c-15243bd84303",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YffUz97pUiWP",
    "outputId": "6ca6cce8-6fa5-4509-bb26-1df1a99a4021",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRnOvnnMUvNO",
    "outputId": "6d8d4a6d-4483-4d05-ce39-eef4eb50e802",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyLhfNi4VXmR",
    "outputId": "c3b7fa69-9c74-420e-f749-9a817b7b8e9b",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the \"system minors\" string\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEi1bb_6VbUT",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8QNvFM8VenA",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# query_document = 'system engineering'.split()\n",
    "# query_bow = dictionary.doc2bow(query_document)\n",
    "# sims = index[tfidf[query_bow]]\n",
    "# print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdIEWatyVgoQ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "#     print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UfTLKgI1ViK1",
    "outputId": "27da26f9-d8b3-4222-ed07-77baaeba2ad2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# sentences只需要是一个可迭代对象就可以\n",
    "model = Word2Vec(processed_corpus, min_count=1)  # 执行这一句的时候就是在训练模型了\n",
    "model.wv.save_word2vec_format('./question.vector')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upBwgXQrVj2P",
    "outputId": "ccdd72af-9085-492a-e3ff-7843374324f8",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for i, word in enumerate(model.wv.vocab):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGHEYw8YVmw6",
    "outputId": "5234eec8-7777-485c-ad9e-72de0566ddcc",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('human', 'production'),   # a minivan is a kind of car\n",
    "    ('human', 'inference'),   # still a wheeled vehicle\n",
    "    ('human', 'representative'),  # ok, no wheels, but still a vehicle\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2,model.wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVmrSaWUBc2P",
    "outputId": "70e914e6-5395-4cfd-83e8-b2d1d741f5ca",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U torch==1.8.0 torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-qTV3ZX8ALs",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import os\n",
    "from torch.nn import init\n",
    "from torchtext.legacy import data\n",
    "from torchtext.vocab import Vectors\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2yspSMo8DRW"
   },
   "source": [
    "### data processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzuAlgY77_-3",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 分词\n",
    "def tokenizer(text): \n",
    "    return [word for word in text if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTcteaqe8MfS",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVrd9KAG8O4d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "396671dd-cf5f-4f72-b1a1-08a09d9303b0",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "text = data.Field(sequential=True,\n",
    "                  lower=True,\n",
    "                  tokenize=tokenizer,\n",
    "                  stop_words=stop_words)\n",
    "label = data.Field(sequential=False)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nl-nFtE08Qck",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train, val = data.TabularDataset.splits(\n",
    "    path='data/',\n",
    "    skip_header=True,\n",
    "    train='train.tsv',\n",
    "    validation='validation.tsv',\n",
    "    format='tsv',\n",
    "    fields=[('text', text), ('label', label)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJHr8lUM8R6P",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# print(train[2].text)\n",
    "# print(train[5].__dict__.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elVmCabw8T2-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cf96e459-c1ab-42f5-cb65-263065628eaf",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#加载Google训练的词向量\n",
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./question.vector', binary=False)\n",
    "print(model)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGRvi3ce8VgT",
    "outputId": "681fdb87-c23d-4428-cd37-8f62c747be84",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cache = 'data/.vector_cache'\n",
    "if not os.path.exists(cache):\n",
    "    os.mkdir(cache)\n",
    "vectors = Vectors(name='./question.vector', cache=cache)\n",
    "# 指定Vector缺失值的初始化方式，没有命中的token的初始化方式\n",
    "# vectors.unk_init = nn.init.xavier_uniform_\n",
    "text.build_vocab(train, val, vectors=vectors)#加入测试集的vertor\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9d7E2RE8XBD",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "text.build_vocab(train, val, vectors=Vectors(name='./question.vector'))#加入测试集的vertor\n",
    "label.build_vocab(train, val)\n",
    "\n",
    "embedding_dim = text.vocab.vectors.size()[-1]\n",
    "vectors = text.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_nITVQ28YVN",
    "outputId": "9d08581f-f7b0-40d0-c990-3f4373a61b46",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "text.vocab.freqs.most_common(10)\n",
    "print(text.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDj5dBgS8Zwb",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size=128\n",
    "train_iter, val_iter = data.Iterator.splits(\n",
    "            (train, val),\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            batch_sizes=(batch_size, len(val)), # 训练集设置batch_size,验证集整个集合用于测试\n",
    "    )\n",
    "\n",
    "vocab_size = len(text.vocab)\n",
    "label_num = len(label.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIQpjnyb8a9m",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kkin6lIP8cKQ"
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egqY5RvJ8dh3",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_hiddens, num_layers):\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "        # embedding之后的shape: torch.Size([200, 8, 300])\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings = self.word_embeddings.from_pretrained(\n",
    "            vectors, freeze=False)\n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim,\n",
    "                               hidden_size=num_hiddens,\n",
    "                               num_layers=num_layers,\n",
    "                               batch_first=False,\n",
    "                               bidirectional=True)\n",
    "        # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        self.w_omega = nn.Parameter(torch.Tensor(\n",
    "            num_hiddens * 2, num_hiddens * 2))\n",
    "        self.u_omega = nn.Parameter(torch.Tensor(num_hiddens * 2, 1))\n",
    "        self.decoder = nn.Linear(2*num_hiddens, 2)\n",
    "\n",
    "        nn.init.uniform_(self.w_omega, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.u_omega, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # print(inputs)\n",
    "        # inputs的形状是(seq_len,batch_size)\n",
    "        embeddings = self.word_embeddings(inputs)\n",
    "        # 提取词特征，输出形状为(seq_len,batch_size,embedding_dim)\n",
    "        # rnn.LSTM只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "        outputs, _ = self.encoder(embeddings)  # output, (h, c)\n",
    "        # outputs形状是(seq_len,batch_size, 2 * num_hiddens)\n",
    "        x = outputs.permute(1, 0, 2)\n",
    "        # x形状是(batch_size, seq_len, 2 * num_hiddens)\n",
    "        \n",
    "        # Attention过程\n",
    "        u = torch.tanh(torch.matmul(x, self.w_omega))\n",
    "       # u形状是(batch_size, seq_len, 2 * num_hiddens)\n",
    "        att = torch.matmul(u, self.u_omega)\n",
    "       # att形状是(batch_size, seq_len, 1)\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "       # att_score形状仍为(batch_size, seq_len, 1)\n",
    "        scored_x = x * att_score\n",
    "       # scored_x形状是(batch_size, seq_len, 2 * num_hiddens)\n",
    "        # Attention过程结束\n",
    "        \n",
    "        feat = torch.sum(scored_x, dim=1)\n",
    "       # feat形状是(batch_size, 2 * num_hiddens)\n",
    "        outs = self.decoder(feat)\n",
    "       # out形状是(batch_size, 2)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laeXLPEw8gw3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0d247055-7e1f-48ec-d5cc-7d4bd7fecb71",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim, num_hiddens, num_layers = 100, 64, 10\n",
    "net = BiLSTM_Attention(vocab_size, embedding_dim, num_hiddens, num_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL78IIRq8h6z"
   },
   "source": [
    "\n",
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlBGUlGz8j5e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter,net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_iter):\n",
    "            X, y = batch.text, batch.label\n",
    "         #   X = X.permute(1, 0)\n",
    "            y.data.sub_(1)  #X转置 y为啥要减1\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: # 自定义的模型, 3.13节之后不会用到, 不考虑GPU\n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5l63vJK8liS",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, num_epochs):\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for batch_idx, batch in enumerate(train_iter):\n",
    "            X, y = batch.text, batch.label\n",
    "           # X = X.permute(1, 0)\n",
    "            y.data.sub_(1)  #X转置 y为啥要减1\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print(\n",
    "            'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "            % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n,\n",
    "               test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBvpBmvP8nU0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "outputId": "6f911256-4492-49ae-e509-5195ece07a39",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lr, num_epochs = 0.01, 10\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, val_iter, net, loss, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pvYIFmHMUXc",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "net(torch.tensor([[00, 2, 3, 4, 5, 6, 7, 8, 9, 10]])).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    ""
   ],
   "metadata": {
    "id": "oeIv-m7flKwz"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvBFbCVGELuW"
   },
   "source": [
    "## Single task QG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAMy79c17eLd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "96d5f757-6ea2-4722-a918-473b50fe940b",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cd question_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OqLOYE2eL6w",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3a455647-7fc2-42de-cb20-99d9b7ede8d3",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxoSS2_WEMvx",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pipelines import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFSZiIc0StHY",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "nlp = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "\n",
    "path_files = glob.glob('process/x_others/*.txt')\n",
    "dirt = \"x_others\"\n",
    "total_file_number = len(path_files)\n",
    "print(\"total_file_number: \" + str(total_file_number))\n",
    "begin = 0\n",
    "for i in range(begin, len(path_files)):\n",
    "  print(\"NOW:\" + str(i+1) + \" / \" + str(total_file_number))\n",
    "  nlp(dirt, \"x_others_\" + str(i) + \".txt\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "id": "FA5uxUBmjvNf",
    "outputId": "51e329d1-b657-4ea2-c1e0-e34f2b9b6299",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZy5F8sjSv2W",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# path_files = glob.glob('process/0_prevention_and_control/*.txt')\n",
    "# dirt = \"0_prevention_and_control\"\n",
    "# total_file_number = len(path_files)\n",
    "# print(\"total_file_number: \" + str(total_file_number))\n",
    "# begin = 0\n",
    "# for i in range(begin, len(path_files)):\n",
    "#   print(\"NOW:\" + str(i+1) + \" / \" + str(total_file_number))\n",
    "#   nlp(dirt, \"0_prevention_and_control_\" + str(i) + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW3xxt88l3eu",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# path_files = glob.glob('process/1_health_and_medicine/*.txt')\n",
    "# dirt = \"1_health_and_medicine\"\n",
    "# total_file_number = len(path_files)\n",
    "# print(\"total_file_number: \" + str(total_file_number))\n",
    "# begin = 0\n",
    "# for i in range(begin, len(path_files)):\n",
    "#   print(\"NOW:\" + str(i+1) + \" / \" + str(total_file_number))\n",
    "#   nlp(dirt, \"1_health_and_medicine_\" + str(i) + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EU1vtmtl4Es",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# path_files = glob.glob('process/2_event_and_timeline/*.txt')\n",
    "# dirt = \"2_event_and_timeline\"\n",
    "# total_file_number = len(path_files)\n",
    "# print(\"total_file_number: \" + str(total_file_number))\n",
    "# begin = 0\n",
    "# for i in range(begin, len(path_files)):\n",
    "#   print(\"NOW:\" + str(i+1) + \" / \" + str(total_file_number))\n",
    "#   nlp(dirt, \"2_event_and_timeline_\" + str(i) + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9s4iOshl4ev",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# path_files = glob.glob('process/3_character/*.txt')\n",
    "# dirt = \"3_character\"\n",
    "# total_file_number = len(path_files)\n",
    "# print(\"total_file_number: \" + str(total_file_number))\n",
    "# begin = 0\n",
    "# for i in range(begin, len(path_files)):\n",
    "#   print(\"NOW:\" + str(i+1) + \" / \" + str(total_file_number))\n",
    "#   nlp(dirt, \"3_character_\" + str(i) + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hnQboP1l5uD",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# path_files = glob.glob('process/4_epidemic/*.txt')\n",
    "# dirt = \"4_epidemic\"\n",
    "# total_file_number = len(path_files)\n",
    "# print(\"total_file_number: \" + str(total_file_number))\n",
    "# begin = 0\n",
    "# for i in range(begin, len(path_files)):\n",
    "#   print(\"NOW:\" + str(i+1) + \" / \" + str(total_file_number))\n",
    "#   nlp(dirt, \"4_epidemic_\" + str(i) + \".txt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZioXMvwhZhEw",
    "5bUO9ffkCUD6",
    "MDLcAEMOYa5S",
    "i3pDcCxbTEhz",
    "j2yspSMo8DRW",
    "MvBFbCVGELuW"
   ],
   "machine_shape": "hm",
   "name": "“question_generation”的副本",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}